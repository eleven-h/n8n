# 🕷️ 爬虫1.0 工作流说明文档

文件：`爬虫1.0.json`

## 1. 这个工作流做什么？

给定一个网站的 `sitemap.xml` 链接，工作流会：
- 拉取并解析 sitemap，筛选出符合条件的产品详情页 URL（基于白名单前缀）
- 将 URL 发送至外部抓取服务获取页面 HTML
- 从 HTML 中提取 `<head>` 与 `#nearstream-content` 主区块，清理、规整为适合 RAG 的文本
- 生成文档指纹（`docId`、`contentHash`）
- 通过 AI 节点对内容结构化清洗，导出文本文件
- 将内容送入向量库（Pinecone）以便后续检索

## 2. 输入与输出

- 输入方式：Chat 窗口中发送包含 `sitemap.xml` 的 URL（由 `When chat message received` 获取 `chatInput`）
- 关键输出：
  - 清洗后的文本文件（转换为文件节点生成）
  - Pinecone 向量库中的文档向量（需配置好索引与凭证）

## 3. 节点与数据流（按执行顺序）

1) `When chat message received`（Chat Trigger）
- 从聊天输入读取 `chatInput`（预期为 sitemap.xml 的 URL）

2) `HTTP Request3`
- 请求 `chatInput` 指向的 URL，获取 sitemap XML

3) `XML1`
- 解析 XML，得到 `urlset.url` 列表

4) `Filter`（Code）
- 脚本功能：
  - 归一化 URL（去 hash、去 UTM、去尾部斜杠）
  - 使用 `productsName` 白名单过滤，只保留目标产品路径
  - 去重
- 输出形如：`{ urls: [...] }`

5) `Split Out1`
- 将 `urls` 数组拆成多条 item

6) `Limit1`
- 限制最多处理 100 条（可按需调整）

7) `Loop Over Items1`
- 为每条 URL 建立循环批处理

8) `HTTP Request4`
- POST 到外部抓取服务（示例：`http://49.51.248.71:11235/crawl`）
- Body：`urls[0] = {{ $json.loc }}`（对当前 URL 发起抓取）

9) `Wait2`
- 等待 2s，避免对外部抓取服务发送过于密集的请求
- 回到 `Loop Over Items1` 继续下一条 URL

10) `HTML`
- 对抓取返回结果的 `results[0].html` 执行选择器提取：
  - `head` → `html`
  - `#nearstream-content` → `html`

11) `Code`
- 处理逻辑：
  - 从 `head` 中提取 SEO 相关标签（title/meta/link/script ld+json）
  - 清理 `content`：保留 `<img>` 属性，其他标签去属性，移除注释（示例中注释函数留空，可按需完善）
  - 组合为 `data`；附带 `url` 与 `name`（URL 最后一段）
- 该节点设置了 `onError: continueErrorOutput`，单条失败不会中断全流程

12) `Build Doc Fingerprint`（Code）
- 生成指纹：
  - `docId = sha1(url)`
  - `contentHash = sha256(data)`
  - 同时再次对 URL 做 canonicalize

13) `AI Agent1`（LangChain Agent）
- 说明：将 `data` 包入 `<html>...</html>`，按预设 Prompt 将页面内容整理为更适合 RAG 的结构化文本（英文输出）
- 语言模型：`Google Gemini Chat Model1`

14) `Convert to File1`
- 将 AI 输出转为文件（txt），文件名：ISO 时间戳

15) `Pinecone Vector Store`
- 模式：`insert`
- 目标索引：`nearstream-content-0810`（示例值，需替换为你的索引）
- 依赖向量与文档装载：本工作流中已包含 `Default Data Loader`、`Recursive Character Text Splitter`、`Embeddings Google Gemini` 等节点用于 LangChain 文档/切分/嵌入，但请注意：
  - 你需要根据自身数据流将“文件输出”对接到 `Default Data Loader`（document）再到 `Text Splitter`（chunk）再到 `Embeddings`（embedding），最终连到 `Pinecone` 的相应输入（`ai_document`/`ai_textSplitter`/`ai_embedding`）
  - 目前 JSON 中这三者已与 Pinecone 建立了 AI 通道连接，但仍需确保上游数据与二进制文件正确传递到 `Default Data Loader`

> 提示：如果你只想先跑通“抓取→清洗→生成文本文件”，可以暂时断开 Pinecone 相关节点。

## 4. 运行前的配置清单

- 外部抓取服务：
  - `HTTP Request4.url` 替换为你的爬虫服务地址
  - 校验 Body 参数名称与服务接口一致（示例为 `urls[0]`）

- 过滤规则：
  - `Filter` 节点内的 `domain` 与 `productsName` 为你的站点与产品路径前缀
  - 不同站点请更新匹配逻辑与白名单表

- AI 模型与凭证：
  - `Google Gemini Chat Model1` 需要配置 API Key/凭证

- 向量库（可选）：
  - `Pinecone Vector Store` 配置索引名称与凭证
  - 确认 `Default Data Loader`、`Text Splitter`、`Embeddings` 的参数满足你的需求

## 5. 如何运行

1) 打开 n8n 的 Chat 窗口（或相关触发界面）
2) 发送一条消息，内容为目标 `sitemap.xml` 的 URL
3) 观察执行数据：
   - `HTTP Request3` 是否成功返回 XML
   - `Filter` 是否筛出目标 URL
   - 循环内 `HTTP Request4` 是否得到 HTML
   - `HTML`/`Code` 是否获得结构化 `data`
   - `AI Agent1` 是否产出文本
   - `Convert to File1` 是否生成文件
   - 若使用 Pinecone，确认入库成功

## 6. 参数与可调节项

- `Limit1.maxItems`：单次运行最大处理 URL 数量
- `Wait2.amount`：抓取两次之间的等待秒数（节流）
- `Filter` 中 `canonicalize`/白名单逻辑：对新站点进行调整
- `AI Agent1` 的 Prompt：根据期望的 RAG 结构进行微调
- `Text Splitter` 的 `chunkOverlap`、`chunkSize`：影响向量化粒度

## 7. 常见问题与排错

- 抓取服务返回为空/失败：
  - 检查 `HTTP Request4` 的 URL、Body、鉴权与 CORS/防火墙配置
  - 增加 `Wait2` 间隔，避免触发对方限流

- 解析不到内容：
  - 检查 `HTML` 节点选择器（`#nearstream-content` 是否存在）
  - 修改为更合适的 DOM 选择器

- AI 输出不理想：
  - 按“单步调试”逐步调整 Prompt
  - 固定输出格式（JSON/Markdown）以稳定下游处理

- 向量库未入库：
  - 确保文档二进制正确传入 `Default Data Loader`
  - 确认 `Embeddings` 与 Pinecone 凭证/索引正确
  - 查看 Pinecone 面板是否有写入记录

## 8. 安全与合规

- 请确认你对目标站点拥有抓取/处理权限，遵守站点 `robots.txt` 及相关条例
- 合理设置请求频率，避免对目标站点造成压力

## 9. 二次开发建议

- 将“过滤逻辑”抽离为配置（如从表格读取白名单）
- 将“抓取服务”替换为自建服务/成熟爬虫（含失败重试、代理池、渲染等）
- 将“清洗模板”与“RAG 分块参数”参数化，便于多站点复用

---

如需我为本工作流自动连通 `Convert to File → Default Data Loader → Text Splitter → Embeddings → Pinecone` 的数据线，并补充必要参数，请告诉我你的向量库与模型配置，我可以直接帮你完善。
